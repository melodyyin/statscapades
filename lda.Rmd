---
title: "Latent Dirichlet allocation with beauty reviews"
author: "Melody"
date: "August 9, 2015"
output: html_document
---

A few weeks ago, I scraped some comments from [MakeupAlley](www.makeupalley.com) (MUA), which is not only the largest but also the most trusted beauty reviews database. For most beauty-related products, "*item name* makeupalley" is one of the top suggested queries on Google. 

I used Python and **BeautifulSoup** to scrape the reviews of 10 random items, each with around 100 reviews each and I loaded them in a txt file. The resulting dataset is quite small (~1MB), but I thought it was a good size for playing around with a text analysis model I had heard about on one of my favorite podcasts (shoutout to [Talking Machines](http://www.thetalkingmachines.com/)) - Latent Dirichlet allocation (LDA). The R packages I am using are **tm**, **slam** and **topicmodels**. I found the [topicmodels vignette](https://cran.r-project.org/web/packages/topicmodels/vignettes/topicmodels.pdf) to be especially helpful. 

In a nutshell, given a set of documents, LDA can give you the categories of words (i.e., topics) that each document is composed of. For example, a news article could be 80% politics and 20% health. This is useful because if you have a massive amount of unlabeled documents, you can use LDA to help get clues of what each document is about. 

~

Let's take a look at the 10 selected items and read in the comments: 

```{r readin}
names = readLines("names.txt")
comments = readLines("comments.txt")
unique(names)
```

Then, load **tm** and do some basic processing: 

```{r processing, message=FALSE}
library(tm)
c.corpus = Corpus(VectorSource(comments))
c.dtm = DocumentTermMatrix(c.corpus, control=list(stopwords=TRUE, minWordLength=3, removeNumbers=TRUE, removePunctuation=TRUE, tolower=TRUE, stemDocument=TRUE))
dim(c.dtm) 
```

Find the total frequency-inverse document frequency (tfidf), which is the term frequency weighted by the log of number of documents over the number of documents that the word appears in. Basically, it is a metric that indicates how important a word is to a document. We then remove the bottom quartile of words to remove the words that appear infrequently or have little meaning. 

```{r tfidf}
library(slam)
c.tfidf = tapply(c.dtm$v/row_sums(c.dtm)[c.dtm$i], c.dtm$j, mean) * log2(nDocs(c.dtm)/col_sums(c.dtm>0))
c.dtm = c.dtm[,c.tfidf >= quantile(c.tfidf, 0.25)] # get rid of irrelevant terms 
dim(c.dtm) 
```

Now, the actual LDA procedure using Gibbs sampling, discarding the first 100 samples and taking every 100th sample from there until the algorithm has been repeated 2000 times: 

```{r lda, message=FALSE}
library(topicmodels)
k = 10 # for the 10 products
sd = 2015 
c.tm = LDA(c.dtm, k=k, method="Gibbs", control=list(seed=sd, burnin=100, thin=100)) # default 2000 iterations
terms(c.tm, 5)
```

How cool! The algorithm was able to discern the various product types that the 10 items belong to. Topic 1 is about toner (item 1), topic 2 is about face makeup (items 5 and 9), topic 4 is about blush (items 2 and 8), topic 5 is about fragrance (item 10), topic 7 is about body wash (items 4 and 7), topic 9 is about hair conditioner (item 6), and topic 10 is about lip product (item 3). All items are accounted for. Topics 3, 6 and 8 are a little vague and don't seem to lean towards any product category, although this is expected since some items belong in the same category. They could be reactions to these items, however; 3 and 6 sound positive, while 8 is more neutral. 

Here are the results for slightly fewer topics (k=7): 

```{r lda2, echo=FALSE}
k = 7
c.tm = LDA(c.dtm, k=k, method="Gibbs", control=list(seed=sd, burnin=100, thin=100))
terms(c.tm, 5)
```

It appears that those reaction categories have been removed. Finally, we try using 3 topics: 

```{r lda3, echo=FALSE}
k = 3
c.tm = LDA(c.dtm, k=k, method="Gibbs", control=list(seed=sd, burnin=100, thin=100))
terms(c.tm, 5)
```

It's reasonable to name the first topic as makeup and the third as skincare. The second is a little difficult.. perhaps personal hygiene?

Even though I know LDA is used more for determining the topic distribution of a document rather than topic discovery (i.e., we care about the distribution of the top categories per document rather than the single most prevalent topic), it's awesome that the algorithm was able to discern the different product types given very limited set of documents and only the number of categories to look for. 

Let's show an example of the topic distributions (using k=3) of a random comment:

```{r distr}
set.seed(809)
choice = sample(1:1042, 1)
comments[choice]
posterior(c.tm)$topics[choice,]
```

56% skincare, 23% makeup, 22% personal hygiene; this was a review for a toner. 

There were similar results with higher k-value; although the correct category would have the highest value, the rest tended to be all similar. This suggests that perhaps LDA works better on longer documents with a greater set of vocabulary or with a larger set of documents. Or, alternatively, my dataset is not appropriate for LDA analysis since the majority of comments will hover around one or two topics. Nevertheless, LDA is a great algorithm to have in my toolkit and I will certainly come back to this algorithm if I encounter any data that would be suited for its usage! 